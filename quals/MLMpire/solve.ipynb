{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ffdefdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from architecture import load_hf_gpt2_model, fill_mask\n",
    "\n",
    "# --- Configuration ---\n",
    "# A list of potential prompts to try, based on the server log context.\n",
    "PROMPT_CANDIDATES = [\n",
    "    # \"user_password\",\n",
    "    # \"secret_key\",\n",
    "    # \"admin_pass\",\n",
    "    # \"password\",\n",
    "    # \"secret\",\n",
    "    # \"auth_token\",\n",
    "    # \"credentials\",\n",
    "    # \"path=/api/auth/create\",\n",
    "    # \"timestamp=\",\n",
    "    \"log entry related to the flag: AI2025{\"\n",
    "]\n",
    "MAX_LENGTH = 500 # Maximum length of the flag to extract\n",
    "FLAG_IDENTIFIER = \"AI2025{\"\n",
    "\n",
    "def solve():\n",
    "    \"\"\"\n",
    "    Loads the model and iterates through a list of prompts to find and\n",
    "    extract the memorized flag.\n",
    "    \"\"\"\n",
    "    print(\"Loading the MLMpire model...\")\n",
    "    # Load the model and vocabulary wrapper from architecture.py\n",
    "    try:\n",
    "        wrapper = load_hf_gpt2_model(model_path=\"./hf_gpt2_model\", vocab_path=\"vocab.json\")\n",
    "        print(\"Model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        print(\"Please ensure 'config.json' and 'pytorch_model.bin' are in the 'hf_gpt2_model' directory.\")\n",
    "        return\n",
    "\n",
    "    # Iterate through each potential starting prompt\n",
    "    for starting_prompt in PROMPT_CANDIDATES:\n",
    "        print(f\"\\n--- Trying prompt: '{starting_prompt}' ---\")\n",
    "        \n",
    "        extracted_secret = \"\"\n",
    "        current_prompt = starting_prompt\n",
    "\n",
    "        print(\"Extracted String: \", end='', flush=True)\n",
    "\n",
    "        # Iteratively predict the next character up to MAX_LENGTH\n",
    "        for _ in range(MAX_LENGTH):\n",
    "            # Create the prompt with the mask token at the end\n",
    "            prompt_with_mask = current_prompt + \"[MASK]\"\n",
    "\n",
    "            # Get the model's predictions (logits) for the masked position\n",
    "            logits = fill_mask(wrapper, prompt_with_mask)\n",
    "\n",
    "            # Find the token with the highest probability\n",
    "            predicted_token_id = torch.argmax(logits).item()\n",
    "\n",
    "            # Convert the token ID back to a character\n",
    "            predicted_char = wrapper.itos[predicted_token_id]\n",
    "\n",
    "            # The vocabulary includes '}' as a character, which is a good stopping point.\n",
    "            if predicted_char in [\"[PAD]\", \"[SEP]\", \"[CLS]\", \"}\"]:\n",
    "                extracted_secret += predicted_char\n",
    "                print(predicted_char, end='', flush=True)\n",
    "                break\n",
    "            \n",
    "            current_prompt += predicted_char\n",
    "            extracted_secret += predicted_char\n",
    "            print(predicted_char, end='', flush=True)\n",
    "        \n",
    "        # Check if this attempt was successful by looking for the flag identifier\n",
    "        full_result = starting_prompt + extracted_secret\n",
    "        if FLAG_IDENTIFIER in full_result:\n",
    "            print(f\"\\n\\n[SUCCESS] Flag found with this prompt!\")\n",
    "            print(f\"Full extracted string: {full_result}\")\n",
    "            return # Exit the function since we found the flag\n",
    "        else:\n",
    "            print(\"\\n[INFO] Flag not found with this prompt. Trying next...\")\n",
    "\n",
    "    print(\"\\n\\nExtraction complete. Flag not found with any of the provided prompts.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8cb618c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the MLMpire model...\n",
      "Model loaded successfully.\n",
      "\n",
      "--- Trying prompt: 'log entry related to the flag: AI2025{' ---\n",
      "Extracted String: 3oi  smcs =ipquess valu=-1n023\":0-dmet\",\"pw\":\"AI2025{m3m0r1z3d_mask_a1_i3_co0l}\n",
      "\n",
      "[SUCCESS] Flag found with this prompt!\n",
      "Full extracted string: log entry related to the flag: AI2025{3oi  smcs =ipquess valu=-1n023\":0-dmet\",\"pw\":\"AI2025{m3m0r1z3d_mask_a1_i3_co0l}\n"
     ]
    }
   ],
   "source": [
    "solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d56f7cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
